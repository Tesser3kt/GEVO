\section{Linear Systems As Vector Spaces}
\label{sec:linear-systems-as-vector-spaces}

We've defined an \hyperref[def:abstract-vector-space]{abstract vector space}
with the primary goal of accommodating the structure of sets of solutions of
homogeneous linear systems. It might come as a surprise that linear systems
\emph{themselves} -- not just their solutions -- exhibit vector space structure.
Understanding of said structure brings to light many properties of their
solution sets, even.

Looking at the left hand side of linear systems (assembled into matrices), one
immediately sees two sets of vectors -- the \textbf{rows} of the matrix, and the
\textbf{columns}. Enlightened as we have been by the late
\myref{section}{sec:subspaces-and-spans} dealing with spans, we begin to study
the vector spaces given by spans of these two sets. Believe it or not, they're
actually closely related. Choosing a `random' table of numbers, you cannot
prevent its rows bearing a similar structure as the columns. Isn't that weird?

\begin{definition}{Row space}{row-space}
 The \emph{row space} of a matrix is the span of its rows.
\end{definition}

\begin{example}{}{}
 The row space of the matrix
 \[
  \begin{pmatrix}
   0 & 1 & 1\\
   1 & 2 & 3
  \end{pmatrix}
 \]
 is the vector space
 \[
  \spn \left( 
  \begin{pmatrix}
   0\\
   1\\
   1
  \end{pmatrix},
  \begin{pmatrix}
   1\\
   2\\
   3
  \end{pmatrix}
 \right).
 \]
\end{example}

The row space of a matrix is tied to the solution of the corresponding linear
system in a tight manner. Observe that the three `row transformations' defined
just prior to \myref{theorem}{thm:gauss-jordan} really just replace rows with
linear combinations of other rows. That said, they do not alter the row space of
a matrix. We shall formulate this observation as a lemma.

\begin{lemma}{}{row-ops-and-row-space}
 Row transformations (1) - (3) defined above \myref{theorem}{thm:gauss-jordan}
 do not change the row space of a matrix. I.e. if matrix $B$ is a matrix derived
 from $A$ by a series of row operations, then the row space of $A$ equals the
 row space of $B$.
\end{lemma}
\begin{lemproof}
 We go through the row transformations one by one and check that they indeed do
 not shrink or enlarge the row space.

 The operation of swapping two rows obviously doesn't change the row space as
 the span of a set of vectors is independent of their order.

 Multiplying a vector of a set by a non-zero constant also clearly doesn't
 affect the span of the set. 

 Finally, assume $\mathbf{a}_1,\ldots,\mathbf{a}_n$ are the rows of $A$. The
 third row transformation amounts to replacing the row $\mathbf{a}_i$ with the
 row $\mathbf{a}_i + c \cdot \mathbf{a}_j$ for some $j \neq i$ and a constant
 $c \in \R$. Clearly, $\mathbf{a}_i + c \cdot \mathbf{a}_j$ lies in the row
 space of $A$. On the other hand, if $B$ is the resulting matrix with rows
 $\mathbf{a}_1,\ldots,\mathbf{a}_{i-1},\mathbf{a}_i + c \cdot \mathbf{a}_j,
 \mathbf{a}_{i+1},\ldots,\mathbf{a}_n$, then $\mathbf{a}_i$ lies in the row
 space of $B$ as
 \[
  \mathbf{a}_i = 1 \cdot (\mathbf{a}_i + c \cdot \mathbf{a}_j) -c \cdot
  \mathbf{a}_j
 \]
 and $\mathbf{a}_i$ is thus a linear combination of rows of $B$. This concludes
 the proof.
\end{lemproof}

In light of \myref{lemma}{lem:row-ops-and-row-space}, we may wish to formalise
the intuition that Gauss-Jordan elimination in fact finds a \textbf{basis} of
the row space of a matrix as it procedurally nullifies rows that can be
expressed as linear combinations of preceding rows. The following lemma is an
ingredient to that dish.

\begin{lemma}{}{non-zero-rows-li}
 The non-zero rows of a matrix in echelon form are linearly independent.
\end{lemma}
\begin{lemproof}
 Each row of a matrix in echelon form has at least one more leading zero than
 the preceding row. That is, labelling the non-zero rows of the eliminated
 matrix $A$ with $n$ rows as $\mathbf{a}_1,\ldots,\mathbf{a}_k$, consider the
 linear combination
 \[
  r_1 \cdot \mathbf{a}_1 + r_2 \cdot \mathbf{a}_2 + \ldots + r_k \cdot
  \mathbf{a}_k = 0.
 \]
 Rewriting this system in the form of a matrix gives
 \[
  \left(
   \begin{matrix*}[c]
    a_{1,1} & 0 & 0 & \cdots & 0\\
    a_{2,1} & a_{2,2} & 0 & \cdots & 0\\
    a_{3,1} & a_{3,2} & a_{3,3} & \cdots & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    a_{n,1} & a_{n,2} & a_{n,3} & \cdots & a_{n,k}
   \end{matrix*}
   \hspace{1mm}
  \right|
  \left.
   \begin{matrix*}[r]
    0\\
    0\\
    0\\
    \vdots\\
    0
   \end{matrix*}
  \right).
 \]
 We're certain that $a_{i,j} = 0$ if $j > i$ because the $i$-th row
 $\mathbf{a}_i$ (which is now the $i$-th column in the matrix) must have at
 least $i - 1$ leading zeroes.

 Simple back substitution immediately yields $r_1 = r_2 = \ldots = r_k = 0$ and
 thus the row vectors $\mathbf{a}_1,\ldots,\mathbf{a}_k$ are linearly
 independent by \myref{proposition}{prop:linear-independence-zero-vector}.
\end{lemproof}

Let us now take a look at the vector space spanned by the columns of a matrix.
We shall uncover interesting links to the row space.

\begin{definition}{Column space}{column-space}
 The column space of a matrix is the span of its columns.
\end{definition}

\begin{example}{}{}
 The column space of the matrix
 \[
  \begin{pmatrix}
   0 & 1 & 1\\
   1 & 2 & 3
  \end{pmatrix}
 \]
 is the vector space
 \[
  \spn \left( 
   \begin{pmatrix}
    0\\
    1
   \end{pmatrix},
   \begin{pmatrix}
    1\\
    2
   \end{pmatrix},
   \begin{pmatrix}
    1\\
    3
   \end{pmatrix}
  \right).
 \]
\end{example}

There is an obvious reason we care about studying the column space. Picking a
matrix $A = (\tilde{\mathbf{a}}_1 | \cdots | \tilde{\mathbf{a}}_n)$ (i.e. we
label its columns as $\tilde{\mathbf{a}}_1$ up to $\tilde{\mathbf{a}}_n$),
every solution of the corresponding homogeneous linear system assumes the form
\[
 r_1 \cdot \tilde{\mathbf{a}}_1 + r_2 \cdot \tilde{\mathbf{a}}_2 + \ldots +
 r_n \cdot \tilde{\mathbf{a}}_n.
\]
Consequently, the column space is \textbf{exactly} the vector space of
solutions of the homogeneous linear system with matrix $A$.

In order to find a \emph{basis} of a vector space given as a span of some set we
would assemble the spanning vectors into rows of a matrix and then that into
echelon form. Should we thus wish to find the basis for the column space, we
would `assemble the columns of a matrix into rows'. This matrix operation is
called the \emph{transpose}.

\begin{definition}{Transpose of a matrix}{transpose-of-a-matrix}
 Given matrix $A = (\tilde{\mathbf{a}}_1 | \tilde{\mathbf{a}}_2 | \cdots |
 \tilde{\mathbf{a}}_n)$, its \emph{transpose} is the matrix $A^{T}$ with rows
 $\tilde{\mathbf{a}}_1,\tilde{\mathbf{a}}_2,\ldots,\tilde{\mathbf{a}}_n$.
\end{definition}

\begin{example}{}{}
 The transpose of the matrix
 \[
  \begin{pmatrix}
   0 & 1 & 1\\
   1 & 2 & 3
  \end{pmatrix}
 \]
 is the matrix
 \[
  \begin{pmatrix}
   0 & 1\\
   1 & 2\\
   1 & 3
  \end{pmatrix}.
 \]
\end{example}

The circle has gone closed with the following lemma which establishes that row
operations do not alter the column space. Restated, row operations do not change
the solution set of a linear system. That is, in fact, the first theorem of the
book -- \myref{theorem}{thm:gauss-jordan}.

\begin{lemma}{}{row-ops-and-col-space}
 Row transformations (1) - (3) defined above \myref{theorem}{thm:gauss-jordan}
 do not change the column space of a matrix.
\end{lemma}
\begin{lemproof}
 See the mentioned \myref{theorem}{thm:gauss-jordan}.
\end{lemproof}

We are now ready to present an important observation, one that ties together the
dimension of row space to that of the column space. The crux of the matter is
that Gauss-Jordan elimination actually doesn't find only the basis of the row
space, it also finds the basis of the \textbf{column} space. We first illustrate
why this is the case on an example.

\begin{example}{}{row-space-col-space}
 Let us put the matrix
 \[
  A = \begin{pmatrix}
  1 & 2 & 0 & 4\\
  3 & 3 & 1 & 0\\
  7 & 8 & 2 & 4
  \end{pmatrix}
 \]
 into echelon form. Following the algorithm of Gauss-Jordan elimination gives
 \[
  \begin{pmatrix*}[r]
   1 & 2 & 0 & 4\\
   0 & -3 & 1 & -12\\
   0 & 0 & 0 & 0
  \end{pmatrix*}.
 \]
 By \myref{lemma}{lem:non-zero-rows-li}, the first and second rows of the
 eliminated matrix form the basis of the row space of the original matrix. That
 is,
 \[
  \text{the row space of } A = \spn \left( 
  \begin{pmatrix}
   1\\
   2\\
   0\\
   4
  \end{pmatrix},
  \begin{pmatrix}
   0\\
   -3\\
   1\\
   -12
  \end{pmatrix}
  \right).
 \]
 However, by the preceding \myref{lemma}{lem:row-ops-and-col-space}, the
 performed operations also left the column space intact. Can we see the basis
 for the column space in the eliminated matrix? Why, of course we can! The
 columns that correspond to \textbf{free variables} (in this case the third and
 fourth columns) are necessarily linearly dependent on previous columns. The
 reason for that is simple -- their last non-zero entry is a pivot in the same
 row and some previous column, their penultimate non-zero entry is again a pivot
 in the same row as that entry and some previous column, etc. Therefore, for
 every non-zero entry of a free variable column, there exists some previous
 \textbf{pivot} column which has a non-zero entry at the same coordinate.

 It follows that
 \[
  \text{the column space of } A = \spn \left(
   \begin{pmatrix}
    1\\
    0\\
    0
   \end{pmatrix},
   \begin{pmatrix}
    2\\
    -3\\
    0
   \end{pmatrix}
  \right).
 \]
 Observe that the dimension of the column space is equal to that of the row
 space. This is not a coincidence -- the echelon form of $A$ has as many zero
 rows as there are free variables. But that is also exactly the number of
 columns that are linearly dependent on other columns. In other words, the
 dimension of \textbf{both} the row space \textbf{and} the column space is the
 number of pivots.
\end{example}

Let us summarise our findings in the following proposition.

\begin{proposition}{}{row-space-equals-col-space}
 For every matrix $A$, the dimension of the row space of $A$ is equal to the
 dimension of the column space of $A$.
\end{proposition}
\begin{propproof}
 The idea of the proof is pretty much contained in
 \myref{example}{exam:row-space-col-space}. The number of non-zero rows in the
 echelon form of $A$ equals the number of pivots (by definition of a
 \emph{pivot}), and, by \myref{lemma}{lem:non-zero-rows-li}, it also equals the
 dimension of its row space.

 As we've observed in \myref{example}{exam:row-space-col-space}, the number of
 pivots also equals the number of linearly independent columns and thus the
 dimension of the column space of $A$.
\end{propproof}

\begin{definition}{Rank}{rank}
 The \emph{rank} of a matrix $A$ equals the dimension of its row space or its
 column space and is denoted $\rank A$.
\end{definition}

We finish the section strong by explicitly stating the relation between the rank
of a matrix and the solution set of its associated homogeneous linear system.

\begin{theorem}{}{rank-dim-of-solutions}
 Let $A$ be an $m \times n$ matrix. Then, the following claims are equivalent.
 \begin{enumerate}
  \item $\rank M = r$.
  \item The vector space of solutions of the associated homogeneous linear
   system has dimension $n - r$.
 \end{enumerate}
\end{theorem}
\begin{thmproof}
 By \myref{proposition}{prop:row-space-equals-col-space} and the preceding
 example, $\rank M = r$ if and only if Gauss-Jordan elimination process of the
 matrix $A$ ends with $r$ non-zero rows. This in turn happens if and only if the
 number of pivots is exactly $r$. Finally, the number of pivots is $r$ if and
 only if the number of free variables is $n - r$. The number of free variables
 is of course precisely the dimension of the set of solution of the homogeneous
 linear system with matrix $A$.
\end{thmproof}

\begin{definition}{Regular matrix}{regular-matrix}
 An $m \times n$ matrix is called \emph{regular} if $\rank A = n$ (that is, the
 maximum possible). If $A$ is not regular, it is called \emph{singular}.
\end{definition}

\begin{remark}{}{}
 By \myref{theorem}{thm:rank-dim-of-solutions}, a matrix $A$ is \emph{singular}
 if and only if the associated homogeneous linear system has infinitely many
 solutions.
\end{remark}

\begin{corollary}{}{}
 For a \textbf{square} matrix $A$ with $n$ rows and $n$ columns, the following
 claims are equivalent.
 \begin{enumerate}
  \item $\rank A = n$.
  \item $A$ is regular.
  \item The rows of $A$ are linearly independent.
  \item The columns of $A$ are linearly independent.
  \item \textbf{Any} linear system (that is, not just homogeneous) with left
   side $A$ has exactly one solution.
 \end{enumerate}
\end{corollary}
\begin{corproof}
 For $(1) \Leftrightarrow (2)$, we observe that $\rank A = n$ if and only if the
 associated homogeneous linear system has $0$ free variables and thus just one
 solution.
\end{corproof}
