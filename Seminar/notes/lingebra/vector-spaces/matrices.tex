\section{Linear Systems As Vector Spaces}
\label{sec:linear-systems-as-vector-spaces}

We've defined an \hyperref[def:abstract-vector-space]{abstract vector space}
with the primary goal of accommodating the structure of sets of solutions of
homogeneous linear systems. It might come as a surprise that linear systems
\emph{themselves} -- not just their solutions -- exhibit vector space structure.
Understanding of said structure brings to light many properties of their
solution sets, even.

Looking at the left hand side of linear systems (assembled into matrices), one
immediately two sets of vectors -- the \textbf{rows} of the matrix, and the
\textbf{columns}. Enlightened as we have been by the late
\myref{section}{sec:subspaces-and-spans} dealing with spans, we begin to study
the vector spaces given by spans of these two sets. Believe it or not, they're
actually closely related. Choosing a `random' table of numbers, you cannot
prevent its rows bearing a similar structure as the columns. Isn't that weird?

\begin{definition}{Row space}{row-space}
 The \emph{row space} of a matrix is the span of its rows.
\end{definition}

\begin{example}{}{}
 The row space of the matrix
 \[
  \begin{pmatrix}
   0 & 1 & 1\\
   1 & 2 & 3
  \end{pmatrix}
 \]
 is the vector space
 \[
  \spn \left( 
  \begin{pmatrix}
   0\\
   1\\
   1
  \end{pmatrix},
  \begin{pmatrix}
   1\\
   2\\
   3
  \end{pmatrix}
 \right).
 \]
\end{example}

The row space of a matrix is tied to the solution of the corresponding linear
system in a tight manner. Observe that the three `row transformations' defined
just prior to \myref{theorem}{thm:gauss-jordan} really just replace rows with
linear combinations of other rows. That said, they do not alter the row space of
a matrix. We shall formulate this observation as a lemma.

\begin{lemma}{}{row-ops-and-row-space}
 Row transformations (1) - (3) defined above \myref{theorem}{thm:gauss-jordan}
 do not change the row space of a matrix. I.e. if matrix $B$ is a matrix derived
 from $A$ by a series of row operations, then the row space of $A$ equals the
 row space of $B$.
\end{lemma}
\begin{lemproof}
 We go through the row transformations one by one and check that they indeed do
 not shrink or enlarge the row space.

 The operation of swapping two rows obviously doesn't change the row space as
 the span of a set of vectors is independent of their order.

 Multiplying a vector of a set by a non-zero constant also clearly doesn't
 affect the span of the set. 

 Finally, assume $\mathbf{a}_1,\ldots,\mathbf{a}_n$ are the rows of $A$. The
 third row transformation amounts to replacing the row $\mathbf{a}_i$ with the
 row $\mathbf{a}_i + c \cdot \mathbf{a}_j$ for some $j \neq i$ and a constant
 $c \in \R$. Clearly, $\mathbf{a}_i + c \cdot \mathbf{a}_j$ lies in the row
 space of $A$. On the other hand, if $B$ is the resulting matrix with rows
 $\mathbf{a}_1,\ldots,\mathbf{a}_{i-1},\mathbf{a}_i + c \cdot \mathbf{a}_j,
 \mathbf{a}_{i+1},\ldots,\mathbf{a}_n$, then $\mathbf{a}_i$ lies in the row
 space of $B$ as
 \[
  \mathbf{a}_i = 1 \cdot (\mathbf{a}_i + c \cdot \mathbf{a}_j) + (-c) \cdot
  \mathbf{a}_j
 \]
 and $\mathbf{a}_i$ is thus a linear combination of rows of $B$. This concludes
 the proof.
\end{lemproof}

In light of \myref{lemma}{lem:row-ops-and-row-space}, we may wish to formalise
the intuition that Gauss-Jordan elimination in fact finds a \textbf{basis} of
the row space of a matrix as it procedurally nullifies rows that can be
expressed as linear combinations of preceding rows. The following lemma is an
ingredient to that dish.

\begin{lemma}{}{}
 The non-zero rows of a matrix in echelon form are linearly independent.
\end{lemma}
\begin{lemproof}
 Each row of a matrix in echelon form has at least one more leading zero than
 the preceding row. That is, labelling the non-zero rows of the eliminated
 matrix $A$ with $n$ rows as $\mathbf{a}_1,\ldots,\mathbf{a}_k$, we have
\end{lemproof}

