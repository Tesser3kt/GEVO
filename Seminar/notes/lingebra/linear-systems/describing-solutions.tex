\section{Describing Solution Sets of Linear Systems}
\label{sec:describing-solution-sets-of-linear-systems}

In \myref{section}{sec:visualization}, we studied specific (simple) classes of
linear systems and touched upon a few important concepts, including, but not
limited to, \emph{parameters}, \emph{free variables}, \emph{underdetermined} and
\emph{overdetermined} systems.

We continue down this road and bring a general description of solution sets of
linear systems. Before we formulate the result we shall endeavour to prove in
this section, we introduce a few pieces of notation which are going to allow us
to manipulate linear systems more efficiently. Do note that behind these mere
`pieces of notation' there lies hidden a much deeper geometric meaning, to be
uncovered in later chapters.

\begin{definition}{Matrix}{matrix}
 An $m \times n$ \emph{matrix} is an array of numbers with $m$ rows and $n$
 columns. The numbers are then called \emph{entries} of the matrix.
\end{definition}

Matrices allow us to write linear systems in a much more succinct manner. For
example, the system
\[
 \begin{array}{r c r c r}
  -x & + & y & = & 2\\
  2x & - & 2y & = & 5
 \end{array}
\]
can be written using a matrix like this:
\[
 \left(
  \begin{matrix*}[r]
   -1 & 1\\
   2 & -2
  \end{matrix*}
  \hspace{1mm}
 \right|
 \left.
  \begin{matrix*}[r]
   2\\
   5
  \end{matrix*}
 \right),
\]
abusing the fact that the same variables are piled in a single column and each
row is a single linear equation. The bar on the right side simply serves to
divide left sides of the equations from right ones.

Matrices make (amongst other things) Gauss-Jordan elimination easier to perform
and keep track of its progress. The matrix of the eliminated system looks like
this
\[
 \left(
  \begin{matrix*}[r]
   -1 & 1\\
   0 & 0
  \end{matrix*}
  \hspace{1mm}
 \right|
 \left.
  \begin{matrix*}[r]
   2\\
   9
  \end{matrix*}
 \right)
\]
and has been reached by the row operation $\mathtt{II + 2I}$.

Certain matrices are special (for reasons soon to be revealed) and we call them
\emph{vectors}.

\begin{definition}{Vector}{vector}
 A \emph{column vector} is an $n \times 1$ matrix (that is, matrix with a single
 column) and a \emph{row vector} is a $1 \times n$ matrix (a matrix with a
 single row). As column vectors are the `default', we call them simply
 \emph{vectors}.
\end{definition}

There exists an obvious bijection between tuples $(v_1,\ldots,v_n)$ and column
vectors $\mathbf{v} = \begin{psmallmatrix} v_1\\[-4pt]\vdots\\v_n
\end{psmallmatrix}$. Consequently, we say that a vector $\mathbf{v}$ with
entries $v_1,\ldots,v_n$ \emph{solves} a linear equation
\[
 a_1x_1 + a_2x_2 + \ldots + a_nx_n = c
\]
if the tuple $(v_1,\ldots,v_n)$ does.

The addition of vectors and their multiplication by a number are defined
naturally.

\begin{definition}{Adding vectors}{adding-vectors}
 Given vectors
 \[
  \mathbf{u} =
  \begin{pmatrix}
   u_1\\
   u_2\\
   \vdots\\
   u_n
  \end{pmatrix}
  \quad \text{and} \quad 
  \mathbf{v} = 
  \begin{pmatrix}
   v_1\\
   v_2\\
   \vdots\\
   v_n
  \end{pmatrix},
 \]
 their \emph{sum} is defined as the vector
 \[
  \mathbf{u} + \mathbf{v} \coloneqq 
  \begin{pmatrix}
   u_1 + v_1\\
   u_2 + v_2\\
   \vdots\\
   u_n + v_n
  \end{pmatrix}.
 \]
\end{definition}

\begin{definition}{Multiplying vector by a number}{multiplying-vector-by-a-number}
 Given a vector
 \[
  \mathbf{v} = 
  \begin{pmatrix}
   v_1\\
   v_2\\
   \vdots\\
   v_n
  \end{pmatrix}
 \]
 and a number $c$, the \emph{scalar $c$-multiple of $\mathbf{v}$} is the vector
 \[
  c \mathbf{v} \coloneqq 
  \begin{pmatrix}
   cv_1\\
   cv_2\\
   \vdots\\
   cv_n
  \end{pmatrix}.
 \]
 The multiplying number $c$ is often referred to as a \emph{scalar}.
\end{definition}

We further need to discuss the concept of \emph{free variables} and
\emph{parameters}.

In the \hyperref[sec:visualization]{previous section}, we described the solution
set of the system~\eqref{eq:two-vars-one-eq} using three different ways. In each
case, two of the variables were independent and the third was their linear
combination. We style the two independent variables, \emph{parameters}. Vaguely
said, a \emph{parameter} is a variable on the value thereof other variables
depend.

The question arises: `Which variables to choose as \emph{parameters}?' The
answer descends: `Why, of course, my child, choose the \emph{free variables}!'
After the process of Gauss-Jordan elimination, a preceding row always has more
variables present than its neighbour downstairs. Occasionally, the number of
additional variables is larger than one. It is clear that in such cases,
back-substitution cannot determine the values of those additional variables
exactly (as it leads to a linear equation in more than one variable). All save
one of those variables are to be chosen as \emph{parameters} and serve the noble
purpose of describing the value of the last variable standing. Custom dictates
that all but the leftmost variable in such a row are labelled \emph{free} and
the leftmost variable called a \emph{pivot}. In light of this, the heavenly
answer can be decrypted -- the \emph{free} variables shall serve as
\emph{parameters} and the value of the \emph{pivot} written as a linear
combination of free variables.

To understand explicitly the preceding paragraph, consider the eliminated system
\[
 \left(
  \begin{matrix*}[r]
   1 & 2 & -1 & 3\\
   0 & 0 & 1 & 1\\
   0 & 0 & 0 & 0
  \end{matrix*}
  \hspace{1mm}
 \right|
 \left.
  \begin{matrix*}[r]
   1\\
   4\\
   0
  \end{matrix*}
 \right).
\]
Row $\mathtt{II}$ has two more variables than row $\mathtt{III}$ and row
$\mathtt{I}$ also wins by two variables over row $\mathtt{II}$. As the holy text
states, the variable $x_4$ of the fourth column is \emph{free}, whereas $x_3$ is
a \emph{pivot}. Therefore, $x_4$ now serves as a parameter and from row
$\mathtt{II}$ we get the relation
\[
 x_3 = -x_4 + 4.
\]
Row $\mathtt{I}$ brings in a new free variable -- $x_2$ -- and a new pivot --
$x_1$. Using the fact that $x_3$, the pivot from row $\mathtt{II}$, is already
expressed as a linear combination of free variables, we substitute into row
$\mathtt{I}$ to get
\[
 x_1 + 2x_2 - (-x_4 + 4) + 3x_4 = 1.
\]
A tiny bit of cheap computation yields
\[
 x_1 = -2x_2 -4x_4 + 5.
\]
Hence, all the pivots of the systems are expressed as linear combinations of
free variables. The set of solutions of this system can be described as the set
of quadruples $(-2x_2 - 4x_4 + 5, x_2, -x_4 + 4, x_4)$.

Visualisation of the concepts of pivots and free variables is provided in
\myref{figure}{fig:free-and-pivots}.

\begin{figure}[ht]
 \centering
 \begin{tikzpicture}[baseline={(5,0)}]
  \newlength{\mysep}
  \setlength{\mysep}{4mm}
  \node at (-0.5,-0.8) {$\left(\vphantom{\rule{0pt}{1.5cm}}\right.$};
  \node at (4.1,-0.8) {$\left.\vphantom{\rule{0pt}{1.5cm}}\right)$};
  
  % Pivots
  \node[fill=BrickRed,inner sep=.5\mysep] at (0, 0) {};
  \node[fill=BrickRed,inner sep=.5\mysep] at (2\mysep, -\mysep) {};
  \node[fill=BrickRed,inner sep=.5\mysep] at (3\mysep, -2\mysep) {};
  \node[fill=BrickRed,inner sep=.5\mysep] at (6\mysep, -3\mysep) {};
  \node[fill=BrickRed,inner sep=.5\mysep] at (8\mysep, -4\mysep) {};

  % Free variables
  \foreach \x in {1, 4, 5, 7, 9} {
   \node[fill=RoyalBlue,inner sep=.5\mysep] at (\x*\mysep,0) {};
  }
  \foreach \x in {4, 5, 7, 9} {
   \node[fill=RoyalBlue,inner sep=.5\mysep] at (\x*\mysep,-\mysep) {};
  }
  \foreach \x in {4, 5, 7, 9} {
   \node[fill=RoyalBlue,inner sep=.5\mysep] at (\x*\mysep,-2\mysep) {};
  }
  \foreach \x in {7, 9} {
   \node[fill=RoyalBlue,inner sep=.5\mysep] at (\x*\mysep,-3\mysep) {};
  }
  \foreach \x in {9} {
   \node[fill=RoyalBlue,inner sep=.5\mysep] at (\x*\mysep,-4\mysep) {};
  }
  
  % Lower pivots
  \foreach \x in {2, 3, 6, 8} {
   \node[fill=ForestGreen,inner sep=.5\mysep] at (\x*\mysep,0) {};
  }
  \foreach \x in {3, 6, 8} {
   \node[fill=ForestGreen,inner sep=.5\mysep] at (\x*\mysep,-\mysep) {};
  }
  \foreach \x in {6, 8} {
   \node[fill=ForestGreen,inner sep=.5\mysep] at (\x*\mysep,-2\mysep) {};
  }
  \foreach \x in {8} {
   \node[fill=ForestGreen,inner sep=.5\mysep] at (\x*\mysep,-3\mysep) {};
  }
 \end{tikzpicture}
 \caption{Visual depiction of an eliminated matrix. \clr{Red} variables are
  pivots, \clb{blue} ones are free and \clg{green} ones are pivots from lower
  rows.}
 \label{fig:free-and-pivots}
\end{figure}

Using \hyperref[def:vector]{vectors}, the solution set of the currently studied
system can be expressed quite elegantly. First, the quadruple $(-2x_2 - 4x_4 +
5, x_2,-x_4 + 4,x_4)$ corresponds to the column vector
\[
 \begin{pmatrix}
  -2x_2 - 4x_4 + 5\\
  x_2\\
  -x_4 + 4\\
  x_4
 \end{pmatrix}.
\]
This vector can be further broken down into three vectors, two for the free
variables and one for the constants. Explicitly,
\[
 \begin{pmatrix}
  -2x_2 - 4x_4 + 5\\
  x_2\\
  -x_4 + 4\\
  x_4
 \end{pmatrix} = x_2
 \begin{pmatrix}
  -2\\
  1\\
  0\\
  0
 \end{pmatrix} + x_4
 \begin{pmatrix}
  -4\\
  0\\
  -1\\
  1
 \end{pmatrix} + 
 \begin{pmatrix}
  5\\
  0\\
  4\\
  0
 \end{pmatrix}.
\]
Take note that the last vector is a \emph{particular} solution of the system
obtained by setting $x_2 = x_4 = 0$. Adding random multiples of the vectors
\[
 \begin{pmatrix}
  -2\\
  1\\
  0\\
  0
 \end{pmatrix} \quad \text{and} \quad 
 \begin{pmatrix}
  -4\\
  0\\
  -1\\
  1
 \end{pmatrix}
\]
to this particular solution generates more solutions of the system.

Let's make another example, shall we? In this eliminated system of two equations
in three variables,
\[
 \left(
  \begin{matrix*}[r]
   1 & 1 & -1\\
   0 & 0 & 2 \\
  \end{matrix*}
  \hspace{1mm}
 \right|
 \left.
  \begin{matrix*}[r]
   2\\
   2
  \end{matrix*}
 \right),
\]
the variables $x_1$ and $x_3$ are pivots and $x_2$ is free. Judging from the
previous example, we should be able to express its solution as $\mathbf{u} + x_2
\mathbf{v}$ where $\mathbf{u}$ and $\mathbf{v}$ are vectors and, furthermore,
$\mathbf{u}$ is some particular solution of the system at hand.

Indeed, choosing $x_2$ to be a parameter, back-substitution yields $x_3 = 1$ and
$x_1 = 2 - x_2 + x_3 = -x_2 + 3$. Hence, every vector of the shape
\[
 \begin{pmatrix}
  -x_2 + 3\\
  x_2\\
  1
 \end{pmatrix} = x_2
 \begin{pmatrix}
  -1\\
  1\\
  0
 \end{pmatrix} + 
 \begin{pmatrix}
  3\\
  0\\
  1
 \end{pmatrix}
\]
solves the system.

% --------------

We're now equipped to formulate a result about the `shape' of a linear system's
solution set with a rather far-reaching importance.

\begin{theorem}{Solution set of a linear system}{solution-set-of-a-linear-system}
 The solution set of every linear system can be written in the form
 \[
  \{\mathbf{u} + t_1 \mathbf{v_1} + t_2 \mathbf{v_2} + \ldots +
  t_l\mathbf{v_l}\},
 \]
 where $\mathbf{u}$ is a particular solution, $\mathbf{v_1},\ldots,\mathbf{v_l}$
 are vectors and $t_1,\ldots,t_l$ are parameters corresponding to the free
 variables of the eliminated system.
\end{theorem}

Before the proof, we formulate an immediate corollary.

\begin{corollary}{Number of solutions of a linear system}{number-of-solutions-of-a-linear-system}
 Every linear system has zero, one or infinitely many solutions.
\end{corollary}
\begin{corproof}
 Referring to the form of the solution set of a linear system from
 \myref{theorem}{thm:solution-set-of-a-linear-system}, we distinguish three
 cases:
 \begin{enumerate}
  \item The vector $\mathbf{u}$ doesn't exist, therefore the system has \emph{no
   solution}.
  \item The vector $\mathbf{u}$ exists and there are no free variables (only
   pivots) in the eliminated system. In this case, the solution is
   \emph{unique}.
  \item The vector $\mathbf{u}$ exists and there is at least one free variable
   to be found in the eliminated system. In this case, the substitution of any
   number in place of the free variables generates a solution. Hence, there are
   \emph{infinitely many}.
 \end{enumerate}
\end{corproof}

On our way to the proof of \myref{theorem}{thm:solution-set-of-a-linear-system},
we make a preparatory step. We call a linear system \emph{homogeneous} if the
right side of its every equation is $0$. Concretely, a \emph{homogeneous} linear
system assumes the form
\[
 \begin{array}{r c r c c c r c r}
  a_{11}x_1 & + & a_{12}x_2 & + & \ldots & + & a_{1n}x_n & = & 0\\
  a_{21}x_1 & + & a_{22}x_2 & + & \ldots & + & a_{2n}x_n & = & 0\\
            &   &           &   &        &   &           & \vdots & \\
  a_{m1}x_1 & + & a_{m2}x_2 & + & \ldots & + & a_{mn}x_n & = & 0.
 \end{array}
\]
Notice that this system always has at least one solution, namely the vector
$\mathbf{0}$ -- the vector whose every entry is $0$. We shall first prove the
following proposition.

\begin{proposition}{Solution set of a homogeneous linear system}{solution-set-of-a-homogeneous-linear-system}
 The solution set of a \emph{homogeneous} linear system can be written in the
 form
 \[
  \{t_1 \mathbf{v_1} + t_2 \mathbf{v_2} + \ldots + t_l\mathbf{v_l}\},
 \]
 where $\mathbf{v_1},\ldots,\mathbf{v_l}$ are vectors and $t_1,\ldots,t_l$ are
 parameters corresponding to the free variables of the eliminated system.
\end{proposition}
\begin{propproof}
 We consider a homogeneous linear system as above:
\begin{equation}
 \label{eq:hom-lin-sys}
 \begin{array}{r c r c c c r c r}
  a_{1,1}x_1 & + & a_{1,2}x_2 & + & \ldots & + & a_{1,n}x_n & = & 0\\
  a_{2,1}x_1 & + & a_{2,2}x_2 & + & \ldots & + & a_{2,n}x_n & = & 0\\
            &   &           &   &        &   &           & \vdots & \\
  a_{m,1}x_1 & + & a_{m,2}x_2 & + & \ldots & + & a_{m,n}x_n & = & 0.
 \end{array}
\end{equation}
 Firstly, in the light of \myref{theorem}{thm:gauss-jordan}, we may assume that
 the system has been reduced to echelon form. We shall prove that every pivot
 can be written as a linear combination of free variables by induction on the
 number $k$ of rows (counting from the bottom) already substituted into. This
 basically mimics the traditional back-substitution process.

 Without loss of generality, we may also assume that no rows full of zeroes are
 left at the bottom of the system, as those can be ignored. Hence, the last row
 of the eliminated linear system looks like this:
 \[
  a_{m,j}x_j + a_{m,j+1}x_{j+1} + \ldots + a_{m,n}x_n = 0
 \]
 for adequate $1 \leq j \leq n$ and $a_{m,j} \neq 0$. Here, $x_j$ is the pivot
 and $x_{j+1},\ldots,x_n$ are free. This gives the expression
 \[
  x_j = -\frac{1}{a_{m,j}}(a_{m,j+1}x_{j+1} + \ldots + a_{m,n}x_n)
 \]
 of the pivot $x_j$ as a linear combination of the free variables
 $x_{j+1},\ldots,x_n$. So, the result holds for $k = 0$.

 Now, supposing all pivots in the last $k$ rows of the
 system~\eqref{eq:hom-lin-sys} have been written as linear combinations of free
 variables, we write the pivot of the $(m-k)$-th row (or $(k+1)$-st from the
 bottom) also as a linear combination of free variables. Again, there exists
 some smallest $1 \leq i \leq n$ such that $a_{m-k,i} \neq 0$. The $(m-k)$-th
 row is thus
 \[
  a_{m-k,i}x_i + a_{m-k,i+1}x_{i+1} + \ldots + a_{m-k,n}x_n = 0.
 \]
 Performing an analogous computation gives
 \begin{equation}
  \label{eq:row-m-k}
  x_i = - \frac{1}{a_{m-k}}(a_{m-k,i+1}x_{i+1} + \ldots + a_{m-k,n}x_n).
 \end{equation}
 All the variables found on the right side of~\eqref{eq:row-m-k} are either free
 or pivots from lower rows. However, by induction hypothesis, all pivots from
 lower rows have already been expressed as linear combinations of free
 variables. Simple substitution now yields an expression of $x_i$ as a linear
 combination of free variables. With $l$ denoting the number of free variables
 of the eliminated system and splitting the solution vector into a sum of scalar
 multiples of free variables, the result is proven.
\end{propproof}


