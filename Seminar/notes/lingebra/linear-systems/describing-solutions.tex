\section{Describing Solution Sets of Linear Systems}
\label{sec:describing-solution-sets-of-linear-systems}

In \myref{section}{sec:visualizing-linear-systems}, we studied specific (simple)
classes of linear systems and touched upon a few important concepts, including,
but not limited to, \emph{parameters}, \emph{free variables},
\emph{underdetermined} and \emph{overdetermined} systems.

We continue down this road and bring a general description of solution sets of
linear systems. Before we formulate the result we shall endeavour to prove in
this section, we introduce a few pieces of notation which are going to allow us
to manipulate linear systems more efficiently. Do note that behind these mere
`pieces of notation' there lies hidden a much deeper geometric meaning, to be
uncovered in later chapters.

\begin{definition}{Matrix}{matrix}
 An $m \times n$ \emph{matrix} is an array of numbers with $m$ rows and $n$
 columns. The numbers are then called \emph{entries} of the matrix.
\end{definition}

Matrices allow us to write linear systems in a much more succinct manner. For
example, the system
\[
 \begin{array}{r c r c r}
  -x & + & y & = & 2\\
  2x & - & 2y & = & 5
 \end{array}
\]
can be written using a matrix like this:
\[
 \left(
  \begin{matrix*}[r]
   -1 & 1\\
   2 & -2
  \end{matrix*}
  \hspace{1mm}
 \right|
 \left.
  \begin{matrix*}[r]
   2\\
   5
  \end{matrix*}
 \right),
\]
abusing the fact that the same variables are piled in a single column and each
row is a single linear equation. The bar on the right side simply serves to
divide left sides of the equations from right ones.

Matrices make (amongst other things) Gauss-Jordan elimination easier to perform
and keep track of its progress. The matrix of the eliminated system looks like
this
\[
 \left(
  \begin{matrix*}[r]
   -1 & 1\\
   0 & 0
  \end{matrix*}
  \hspace{1mm}
 \right|
 \left.
  \begin{matrix*}[r]
   2\\
   9
  \end{matrix*}
 \right)
\]
and has been reached by the row operation $\mathtt{II + 2I}$.

Certain matrices are special (for reasons soon to be revealed) and we call them
\emph{vectors}.

\begin{definition}{Vector}{vector}
 A \emph{column vector} is an $n \times 1$ matrix (that is, matrix with a single
 column) and a \emph{row vector} is a $1 \times n$ matrix (a matrix with a
 single row). As column vectors are the `default', we call them simply
 \emph{vectors}.
\end{definition}

There exists an obvious bijection between tuples $(v_1,\ldots,v_n)$ and column
vectors $\mathbf{v} = \begin{psmallmatrix} v_1\\[-4pt]\vdots\\v_n
\end{psmallmatrix}$. Consequently, we say that a vector $\mathbf{v}$ with
entries $v_1,\ldots,v_n$ \emph{solves} a linear equation
\[
 a_1x_1 + a_2x_2 + \ldots + a_nx_n = c
\]
if the tuple $(v_1,\ldots,v_n)$ does.

The addition of vectors and their multiplication by a number are defined
naturally.

\begin{definition}{Adding vectors}{adding-vectors}
 Given vectors
 \[
  \mathbf{u} =
  \begin{pmatrix}
   u_1\\
   u_2\\
   \vdots\\
   u_n
  \end{pmatrix}
  \quad \text{and} \quad 
  \mathbf{v} = 
  \begin{pmatrix}
   v_1\\
   v_2\\
   \vdots\\
   v_n
  \end{pmatrix},
 \]
 their \emph{sum} is defined as the vector
 \[
  \mathbf{u} + \mathbf{v} \coloneqq 
  \begin{pmatrix}
   u_1 + v_1\\
   u_2 + v_2\\
   \vdots\\
   u_n + v_n
  \end{pmatrix}.
 \]
\end{definition}

\begin{definition}{Multiplying vector by a number}{multiplying-vector-by-a-number}
 Given a vector
 \[
  \mathbf{v} = 
  \begin{pmatrix}
   v_1\\
   v_2\\
   \vdots\\
   v_n
  \end{pmatrix}
 \]
 and a number $c$, the \emph{scalar $c$-multiple of $\mathbf{v}$} is the vector
 \[
  c \mathbf{v} \coloneqq 
  \begin{pmatrix}
   cv_1\\
   cv_2\\
   \vdots\\
   cv_n
  \end{pmatrix}.
 \]
 The multiplying number $c$ is often referred to as a \emph{scalar}.
\end{definition}

We further need to discuss the concept of \emph{free variables} and
\emph{parameters}.

In the \hyperref[sec:visualizing-linear-systems]{previous section}, we described
the solution set of the system~\eqref{eq:three-vars-one-eq} using three
different ways. In each case, two of the variables were independent and the
third was their linear combination. We style the two independent variables,
\emph{parameters}. Vaguely said, a \emph{parameter} is a variable on the value
whereof other variables depend.

The question arises: `Which variables to choose as \emph{parameters}?' The
answer descends: `Why, of course, my child, choose the \emph{free variables}!'
After the process of Gauss-Jordan elimination, a preceding row always has more
variables present than its neighbour downstairs. Occasionally, the number of
additional variables is larger than one. It is clear that in such cases,
back-substitution cannot determine the values of those additional variables
exactly (as it leads to a linear equation in more than one variable). All save
one of those variables are to be chosen as \emph{parameters} and serve the noble
purpose of describing the value of the last variable standing. Custom dictates
that all but the leftmost variable in such a row are labelled \emph{free} and
the leftmost variable called a \emph{pivot}. In light of this, the heavenly
answer can be decrypted -- the \emph{free} variables shall serve as
\emph{parameters} and the value of the \emph{pivot} be written as a linear
combination of free variables.

To understand explicitly the preceding paragraph, consider the eliminated system
\[
 \left(
  \begin{matrix*}[r]
   1 & 2 & -1 & 3\\
   0 & 0 & 1 & 1\\
   0 & 0 & 0 & 0
  \end{matrix*}
  \hspace{1mm}
 \right|
 \left.
  \begin{matrix*}[r]
   1\\
   4\\
   0
  \end{matrix*}
 \right).
\]
Row $\mathtt{II}$ has two more variables than row $\mathtt{III}$ and row
$\mathtt{I}$ also wins by two variables over row $\mathtt{II}$. As the holy text
states, the variable $x_4$ of the fourth column is \emph{free}, whereas $x_3$ is
a \emph{pivot}. Therefore, $x_4$ now serves as a parameter and from row
$\mathtt{II}$ we get the relation
\[
 x_3 = -x_4 + 4.
\]
Row $\mathtt{I}$ brings in a new free variable -- $x_2$ -- and a new pivot --
$x_1$. Using the fact that $x_3$, the pivot from row $\mathtt{II}$, is already
expressed as a linear combination of free variables, we substitute into row
$\mathtt{I}$ to get
\[
 x_1 + 2x_2 - (-x_4 + 4) + 3x_4 = 1.
\]
A tiny bit of cheap computation yields
\[
 x_1 = -2x_2 -4x_4 + 5.
\]
Thereby, all the pivots of the system are expressed as linear combinations of
free variables. The set of solutions of this system can be described as the set
of quadruples $(-2x_2 - 4x_4 + 5, x_2, -x_4 + 4, x_4)$.

Visualisation of the concepts of pivots and free variables is provided in
\myref{figure}{fig:free-and-pivots}.

\begin{figure}[ht]
 \centering
 \begin{tikzpicture}[baseline={(5,0)}]
  \newlength{\mysep}
  \setlength{\mysep}{4mm}
  \node at (-0.5,-0.8) {$\left(\vphantom{\rule{0pt}{1.5cm}}\right.$};
  \node at (4.1,-0.8) {$\left.\vphantom{\rule{0pt}{1.5cm}}\right)$};
  
  % Pivots
  \node[fill=BrickRed,inner sep=.5\mysep] at (0, 0) {};
  \node[fill=BrickRed,inner sep=.5\mysep] at (2\mysep, -\mysep) {};
  \node[fill=BrickRed,inner sep=.5\mysep] at (3\mysep, -2\mysep) {};
  \node[fill=BrickRed,inner sep=.5\mysep] at (6\mysep, -3\mysep) {};
  \node[fill=BrickRed,inner sep=.5\mysep] at (8\mysep, -4\mysep) {};

  % Free variables
  \foreach \x in {1, 4, 5, 7, 9} {
   \node[fill=RoyalBlue,inner sep=.5\mysep] at (\x*\mysep,0) {};
  }
  \foreach \x in {4, 5, 7, 9} {
   \node[fill=RoyalBlue,inner sep=.5\mysep] at (\x*\mysep,-\mysep) {};
  }
  \foreach \x in {4, 5, 7, 9} {
   \node[fill=RoyalBlue,inner sep=.5\mysep] at (\x*\mysep,-2\mysep) {};
  }
  \foreach \x in {7, 9} {
   \node[fill=RoyalBlue,inner sep=.5\mysep] at (\x*\mysep,-3\mysep) {};
  }
  \foreach \x in {9} {
   \node[fill=RoyalBlue,inner sep=.5\mysep] at (\x*\mysep,-4\mysep) {};
  }
  
  % Lower pivots
  \foreach \x in {2, 3, 6, 8} {
   \node[fill=ForestGreen,inner sep=.5\mysep] at (\x*\mysep,0) {};
  }
  \foreach \x in {3, 6, 8} {
   \node[fill=ForestGreen,inner sep=.5\mysep] at (\x*\mysep,-\mysep) {};
  }
  \foreach \x in {6, 8} {
   \node[fill=ForestGreen,inner sep=.5\mysep] at (\x*\mysep,-2\mysep) {};
  }
  \foreach \x in {8} {
   \node[fill=ForestGreen,inner sep=.5\mysep] at (\x*\mysep,-3\mysep) {};
  }
 \end{tikzpicture}
 \caption{Visual depiction of an eliminated matrix. \clr{Red} variables are
  pivots, \clb{blue} ones are free and \clg{green} ones are pivots from lower
  rows.}
 \label{fig:free-and-pivots}
\end{figure}

Using \hyperref[def:vector]{vectors}, the solution set of the currently studied
system can be expressed quite elegantly. First, the quadruple $(-2x_2 - 4x_4 +
5, x_2,-x_4 + 4,x_4)$ corresponds to the column vector
\[
 \begin{pmatrix}
  -2x_2 - 4x_4 + 5\\
  x_2\\
  -x_4 + 4\\
  x_4
 \end{pmatrix}.
\]
This vector can be further broken down into three vectors, two for the free
variables and one for the constants. Explicitly,
\[
 \begin{pmatrix}
  -2x_2 - 4x_4 + 5\\
  x_2\\
  -x_4 + 4\\
  x_4
 \end{pmatrix} = x_2
 \begin{pmatrix}
  -2\\
  1\\
  0\\
  0
 \end{pmatrix} + x_4
 \begin{pmatrix}
  -4\\
  0\\
  -1\\
  1
 \end{pmatrix} + 
 \begin{pmatrix}
  5\\
  0\\
  4\\
  0
 \end{pmatrix}.
\]
Take note that the last vector is a \emph{particular} solution of the system
obtained by setting $x_2 = x_4 = 0$. Adding random multiples of the vectors
\[
 \begin{pmatrix}
  -2\\
  1\\
  0\\
  0
 \end{pmatrix} \quad \text{and} \quad 
 \begin{pmatrix}
  -4\\
  0\\
  -1\\
  1
 \end{pmatrix}
\]
to this particular solution generates more solutions of the system.

Let's make another example, shall we? In this eliminated system of two equations
in three variables,
\[
 \left(
  \begin{matrix*}[r]
   1 & 1 & -1\\
   0 & 0 & 2 \\
  \end{matrix*}
  \hspace{1mm}
 \right|
 \left.
  \begin{matrix*}[r]
   2\\
   2
  \end{matrix*}
 \right),
\]
the variables $x_1$ and $x_3$ are pivots and $x_2$ is free. Judging from the
previous example, we should be able to express its solution as $\mathbf{u} + x_2
\mathbf{v}$ where $\mathbf{u}$ and $\mathbf{v}$ are vectors and, furthermore,
$\mathbf{u}$ is some particular solution of the system at hand.

Indeed, choosing $x_2$ to be a parameter, back-substitution yields $x_3 = 1$ and
$x_1 = 2 - x_2 + x_3 = -x_2 + 3$. Hence, every vector of the shape
\[
 \begin{pmatrix}
  -x_2 + 3\\
  x_2\\
  1
 \end{pmatrix} = x_2
 \begin{pmatrix}
  -1\\
  1\\
  0
 \end{pmatrix} + 
 \begin{pmatrix}
  3\\
  0\\
  1
 \end{pmatrix}
\]
solves the system.

% --------------

We're now equipped to formulate a result about the `shape' of a linear system's
solution set with a rather far-reaching importance.

\begin{theorem}{Solution set of a linear system}{solution-set-of-a-linear-system}
 The solution set of every linear system can be written in the form
 \[
  \{\mathbf{u} + t_1 \mathbf{v_1} + t_2 \mathbf{v_2} + \ldots +
  t_l\mathbf{v_l}\},
 \]
 where $\mathbf{u}$ is a particular solution, $\mathbf{v_1},\ldots,\mathbf{v_l}$
 are vectors and $t_1,\ldots,t_l$ are parameters corresponding to the free
 variables of the eliminated system.
\end{theorem}

Before the proof, we formulate an immediate corollary.

\begin{corollary}{Number of solutions of a linear system}{number-of-solutions-of-a-linear-system}
 Every linear system has zero, one or infinitely many solutions.
\end{corollary}
\begin{corproof}
 Referring to the form of the solution set of a linear system from
 \myref{theorem}{thm:solution-set-of-a-linear-system}, we distinguish three
 cases:
 \begin{enumerate}
  \item The vector $\mathbf{u}$ doesn't exist, therefore the system has \emph{no
   solution}.
  \item The vector $\mathbf{u}$ exists and there are no free variables (only
   pivots) in the eliminated system. In this case, the solution is
   \emph{unique}.
  \item The vector $\mathbf{u}$ exists and there is at least one free variable
   to be found in the eliminated system. In this case, the substitution of any
   number in place of the free variables generates a solution. Hence, there are
   \emph{infinitely many}.
 \end{enumerate}
\end{corproof}

On our way to the proof of \myref{theorem}{thm:solution-set-of-a-linear-system},
we make a preparatory step. We call a linear system \emph{homogeneous} if the
right side of its every equation is $0$. Concretely, a \emph{homogeneous} linear
system assumes the form
\[
 \begin{array}{r c r c c c r c l}
  a_{11}x_1 & + & a_{12}x_2 & + & \ldots & + & a_{1n}x_n & = & 0\\
  a_{21}x_1 & + & a_{22}x_2 & + & \ldots & + & a_{2n}x_n & = & 0\\
            &   &           &   &        &   &           & \vdots & \\
  a_{m1}x_1 & + & a_{m2}x_2 & + & \ldots & + & a_{mn}x_n & = & 0.
 \end{array}
\]
Notice that this system always has at least one solution, namely the vector
$\mathbf{0}$ -- the vector whose every entry is $0$. We shall first prove the
following proposition.

\begin{proposition}{Solution set of a homogeneous linear system}{solution-set-of-a-homogeneous-linear-system}
 The solution set of a \emph{homogeneous} linear system can be written in the
 form
 \[
  \{t_1 \mathbf{v_1} + t_2 \mathbf{v_2} + \ldots + t_l\mathbf{v_l}\},
 \]
 where $\mathbf{v_1},\ldots,\mathbf{v_l}$ are vectors and $t_1,\ldots,t_l$ are
 parameters corresponding to the free variables of the eliminated system.
\end{proposition}
\begin{propproof}
 We consider a homogeneous linear system as above:
\begin{equation}
 \label{eq:hom-lin-sys}
 \begin{array}{r c r c c c r c l}
  a_{1,1}x_1 & + & a_{1,2}x_2 & + & \ldots & + & a_{1,n}x_n & = & 0\\
  a_{2,1}x_1 & + & a_{2,2}x_2 & + & \ldots & + & a_{2,n}x_n & = & 0\\
            &   &           &   &        &   &           & \vdots & \\
  a_{m,1}x_1 & + & a_{m,2}x_2 & + & \ldots & + & a_{m,n}x_n & = & 0.
 \end{array}
\end{equation}
 Firstly, in the light of \myref{theorem}{thm:gauss-jordan}, we may assume that
 the system has been reduced to echelon form. We shall prove that every pivot
 can be written as a linear combination of free variables by induction on the
 number $k$ of rows (counting from the bottom) already substituted into. This
 approach basically mimics and formalises the traditional back-substitution
 process.

 Without loss of generality, we may also assume that no rows full of zeroes are
 left at the bottom of the system, as those can be ignored. Hence, the last row
 of the eliminated linear system looks like this:
 \[
  a_{m,j}x_j + a_{m,j+1}x_{j+1} + \ldots + a_{m,n}x_n = 0
 \]
 for adequate $1 \leq j \leq n$ and $a_{m,j} \neq 0$. Here, $x_j$ is the pivot
 and $x_{j+1},\ldots,x_n$ are free. This gives the expression
 \[
  x_j = -\frac{1}{a_{m,j}}(a_{m,j+1}x_{j+1} + \ldots + a_{m,n}x_n)
 \]
 of the pivot $x_j$ as a linear combination of the free variables
 $x_{j+1},\ldots,x_n$. So, the result holds for $k = 0$.

 Now, supposing all pivots in the last $k$ rows of the
 system~\eqref{eq:hom-lin-sys} have been expressed as linear combinations of
 free variables, we write the pivot of the $(m-k)$-th row (or $(k+1)$-st from
 the bottom) also as a linear combination of free variables. Again, there exists
 some smallest $1 \leq i \leq n$ such that $a_{m-k,i} \neq 0$. The $(m-k)$-th
 row is thus
 \[
  a_{m-k,i}x_i + a_{m-k,i+1}x_{i+1} + \ldots + a_{m-k,n}x_n = 0.
 \]
 Performing an analogous computation gives
 \begin{equation}
  \label{eq:row-m-k}
  x_i = - \frac{1}{a_{m-k}}(a_{m-k,i+1}x_{i+1} + \ldots + a_{m-k,n}x_n).
 \end{equation}
 All the variables found on the right side of~\eqref{eq:row-m-k} are either free
 or pivots from lower rows. However, by the induction hypothesis, all pivots
 from lower rows have already been expressed as linear combinations of free
 variables. Simple substitution now yields an expression of $x_i$ as a linear
 combination of free variables. With $l$ denoting the number of free variables
 of the eliminated system and with the solution vector having been split into a
 sum of scalar multiples of free variables, the result is proven.
\end{propproof}

\begin{remark}{}{homogeneous-solutions}
 By \myref{proposition}{prop:solution-set-of-a-homogeneous-linear-system} above,
 a \emph{homogeneous linear system} has either one or infinitely many solutions
 since the $n$-tuple $(0,0,\ldots,0)$ always solves it.
\end{remark}

\begin{example}{}{homogeneous-linear-1}
 The echelon form of the homogeneous linear system
 \[
  \left(
   \begin{matrix*}[r]
    1 & 4 & 2\\
    -2 & -3 & 1\\
    3 & 7 & 1
   \end{matrix*}
   \hspace{1mm}
  \right|
  \left.
   \begin{matrix*}[r]
    0\\
    0\\
    0
   \end{matrix*}
  \right)
 \]
 is equal to
 \[
  \left(
   \begin{matrix*}[r]
    1 & 4 & 2\\
    0 & 5 & 5\\
    0 & 0 & 0
   \end{matrix*}
   \hspace{1mm}
  \right|
  \left.
   \begin{matrix*}[r]
    0\\
    0\\
    0
   \end{matrix*}
  \right).
 \]
 Hence, the variables $x_1$ and $x_2$ are \emph{pivots} and $x_3$ is
 \emph{free}. Back-substitution gives $x_2 = -x_3$ and $x_1 = 2x_3$. The
 solution set of this system is thus given by all the vectors
 \[
  \begin{pmatrix}
   2x_3\\
   -x_3\\
   x_3
  \end{pmatrix} = x_3
  \begin{pmatrix}
   2\\
   -1\\
   1
  \end{pmatrix}.
 \]
 In the notation of
 \myref{proposition}{prop:solution-set-of-a-homogeneous-linear-system}, we'd
 have
 \[
  t_1 = x_3 \quad \text{and} \quad \mathbf{v}_1 =
  \begin{pmatrix}
   2\\
   -1\\
   1
  \end{pmatrix}.
 \]
\end{example}

We've reached the climax of the section -- the proof of
\myref{theorem}{thm:solution-set-of-a-linear-system}. Armed with
\myref{proposition}{prop:solution-set-of-a-homogeneous-linear-system}, it
behoves us to merely work a link between the solution set of a linear system and
its corresponding homogeneous system.

\begin{thmproof}[of \myref{theorem}{thm:solution-set-of-a-linear-system}]
 Let
 \begin{equation}
  \label{eq:lin-sys}
  \begin{array}{r c r c c c r c l}
   a_{1,1}x_1 & + & a_{1,2}x_2 & + & \ldots & + & a_{1,n}x_n & = & c_1\\
   a_{2,1}x_1 & + & a_{2,2}x_2 & + & \ldots & + & a_{2,n}x_n & = & c_2\\
             &   &           &   &        &   &           & \vdots & \\
   a_{m,1}x_1 & + & a_{m,2}x_2 & + & \ldots & + & a_{m,n}x_n & = & c_m.
  \end{array}
 \end{equation}
 be the linear system in question. We proceed to show that its every solution is
 of the form $\mathbf{u} + \mathbf{h}$, where $\mathbf{u}$ is a particular
 solution and $\mathbf{h}$ is a solution of the corresponding homogeneous linear
 system (as in~\eqref{eq:hom-lin-sys}), and that, contrariwise, for every
 solution $\mathbf{h}$ of the homogeneous linear system, the vector $\mathbf{u}
 + \mathbf{h}$ solves the system~\eqref{eq:lin-sys}, assuming $\mathbf{u}$ does.

 Let's start with the former. Denote by $\mathbf{u}$ any fixed solution of
 system~\eqref{eq:lin-sys}. We want to show that any other solution $\mathbf{v}$
 of the same system can be written as $\mathbf{v} = \mathbf{u} + \mathbf{h}$
 where $\mathbf{h}$ solves the corresponding homogeneous linear
 system~\eqref{eq:hom-lin-sys}. For this, it is clearly enough to show that
 $\mathbf{h} \coloneqq \mathbf{v} - \mathbf{u}$ solves the
 system~\eqref{eq:hom-lin-sys}. Substituting $x_i \coloneqq h_i = v_i - u_i$
 into the left side of the $j$-th equation of~\eqref{eq:hom-lin-sys} yields
 \[
  a_{j,1}(v_1 - u_1) + a_{j,2}(v_2 - u_2) + \ldots + a_{j,n}(v_n - u_n),
 \]
 which can be broken into
 \[
  (a_{j,1}v_1 + \ldots + a_{j,n}v_n) - (a_{j,1}u_1 + \ldots + a_{j,n}u_n).
 \]
 As both $\mathbf{u}$ and $\mathbf{v}$ solve~\eqref{eq:lin-sys}, we know that
 \[
  a_{j,1}v_1 + \ldots + a_{j,n}v_n = c_j = a_{j,1}u_1 + \ldots + a_{j,n}u_n,
 \]
 and thus
 \[
  (a_{j,1}v_1 + \ldots + a_{j,n}v_n) - (a_{j,1}u_1 + \ldots + a_{j,n}u_n) = c_j
  - c_j = 0.
 \]
 This is true for all $1 \leq j \leq m$, hence the result.

 As for the inverse inclusion, we must show that $\mathbf{u} + \mathbf{h}$ where
 $\mathbf{h}$ is an arbitrary solution of~\eqref{eq:hom-lin-sys} also
 solves~\eqref{eq:lin-sys}, assuming that $\mathbf{u}$ solves it. This time, we
 substitute $x_i \coloneqq u_i + h_i$ into the left side of the $j$-th equation
 of~\eqref{eq:lin-sys} and get
 \begin{equation}
  \label{eq:part+hom}
  a_{j,1}(u_1 + h_1) + \ldots + a_{j,n}(u_n + h_n) = (a_{j,1}u_1 + \ldots +
  a_{j,n}u_n) + (a_{j,1}h_1 + \ldots + a_{j,n}h_n).
 \end{equation}
 We know that
 \[
  \begin{array}{r c c c r c l}
   a_{j,1}u_1 & + & \ldots & + & a_{j,n}u_n & = & c_j,\\
   a_{j,1}h_1 & + & \ldots & + & a_{j,n}h_n & = & 0.
  \end{array}
 \]
 Thus, the expression~\eqref{eq:part+hom} equals $c_j + 0 = c_j$ and $\mathbf{u}
 + \mathbf{h}$ solves the $j$-th equations of~\eqref{eq:lin-sys}. Again, this
 being true for all $1 \leq j \leq m$ proves this inclusion and with it, the
 theorem.
\end{thmproof}

\begin{example}{}{linear-system-1}
 We change the right side of the system from
 \myref{example}{exam:homogeneous-linear-1} to produce
 \[
  \left(
   \begin{matrix*}[r]
    1 & 4 & 2\\
    -2 & -3 & 1\\
    3 & 7 & 1
   \end{matrix*}
   \hspace{1mm}
  \right|
  \left.
   \begin{matrix*}[r]
    5\\
    0\\
    5
   \end{matrix*}
  \right),
 \]
 and after elimination:
 \[
  \left(
   \begin{matrix*}[r]
    1 & 4 & 2\\
    0 & 5 & 5\\
    0 & 0 & 0
   \end{matrix*}
   \hspace{1mm}
  \right|
  \left.
   \begin{matrix*}[r]
    5\\
    10\\
    0
   \end{matrix*}
  \right).
 \]
 Typical back-substitution yields $x_2 = 2 - x_3$ and $x_1 = -3 + 2x_3$. The
 solution can thus be written as
 \[
  \begin{pmatrix}
   -3\\
   2\\
   0
  \end{pmatrix} + x_3
  \begin{pmatrix}
   2\\
   -1\\
   1
  \end{pmatrix},
 \]
 where $\mathbf{u} = \begin{psmallmatrix} -3 \\ 2 \\ 0 \end{psmallmatrix}$ is a
 particular solution of the system and $\mathbf{h} = x_3\begin{psmallmatrix} 2
 \\ -1 \\ 1 \end{psmallmatrix}$ is for any $x_3$ a solution of the corresponding
 homogeneous linear system from \myref{example}{exam:homogeneous-linear-1}.
\end{example}

\begin{exercise}{}{describing-solution-set}
 Solve each of the systems below using matrix notation. Write the solution in
 the form of \myref{theorem}{thm:solution-set-of-a-linear-system}.
 \[
  \begin{array}[t]{r c r c r}
   3x & + & 6y & = & 18\\
   x & + & 2y & = & 6
  \end{array} \hspace{2em}
  \begin{array}[t]{r c r c r}
   x & + & y & = & 1\\
   x & - & y & = & -1
  \end{array} \hspace{2em}
  \begin{array}[t]{r c r c r c r c r}
   x_1 & + & 2x_2 & - & x_3 & & & = & 3\\
   2x_1 & + & x_2 & & & + & x_4 & = & 4\\
   x_1 & - & x_2 & + & x_3 & + & x_4 & = & 1
  \end{array}
 \]
\end{exercise}

\begin{exercise}{}{conic-section}
 Show that any five points in the plane $\R^2$ lie on a common \emph{conic
 section}, that is, they all satisfy an equation of the form
 \[
  ax^2 + by^2 + cxy + dx + ey + f = 0
 \]
 for some $a,\ldots,f \in \R$.
\end{exercise}

\begin{exercise}{}{homogeneous-system-solutions}
 Prove that if $\mathbf{s}$ and $\mathbf{t}$ are solutions of a homogeneous
 linear system, then so are
 \begin{enumerate}
  \item $\mathbf{s} + \mathbf{t}$,
  \item $3 \mathbf{s}$,
  \item $k \mathbf{s} + m \mathbf{t}$ for any numbers $k,m$.
 \end{enumerate}
 What is wrong with the following argument: `These three show that if a
 homogeneous system has one solution, then it has many solutions -- any multiple
 of a solution is another solution, and any sum of solutions is also a solution
 -- so there are no homogeneous linear systems with exactly one solution.'?
\end{exercise}
